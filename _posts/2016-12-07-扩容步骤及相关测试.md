---
title: Greemplum扩容
date: 2016-12-07 23:00:00
categories:
- Greenplum
tags:
- Greenplum
---

## 1 扩容

扩容分为两种，一种是在当前集群中增加segment实例数，另一种是集群增加物理机并在新增物理机上配置segment实例。

扩容的操作分为3个步骤执行：
1. 生成扩容配置文件
```
# 生成扩容配置文件，交互输入要扩容的host，每个host上扩容的segment数目，以及mirror的方式(grouped, spread)
# 扩容过程中需要生成表，记录扩容运行情况，因此必须指定一个template1以外的数据库
gpexpand -D zlytest
```
2. 扩容节点初始化
```
# 根据配置文件扩容节点初始化
gpexpand -i gpexpand_inputfile_20161109_094342 -D zlytest
```
2. 数据重分布
```
# 在指定时间内完成重分布
gpexpand -D zlytest -d 1:00:00
```

根据官方文档的描述，扩容过程具体进行如下操作：
1. 短暂的scheduled downtime
- 在新的segment hosts上创建数据库及数据库对象
- 在master数据库中创建控制扩容过程的表
- 将所有表的分布策略修改为随机分布
2. 系统重新启动并对外提供服务
- 新增的segment生效并参与查询，但此时数据还在原segment上，新segment上没有数据，此时数据是倾斜的
- 由于分布策略被改为了随机分布，查询规划器生成的查询计划不使用分布键了，此时查询需要较多的data motion，查询效率降低
3. 数据重分布
- alter table将表的分布策略改回原有的分布策略，触发数据重分布
- 所有表的重分布情况记录在1中建立的扩容过程控制表中
- 查询规划器生成的查询计划使用分布键，查询效率提高
4. 所有表重分布完成后，扩容结束



## 2 扩容测试
- 测试扩容节点初始化、数据重分布过程中是否会对业务造成影响

官方文档中有这么一段话，扩容过程中查询不会被中断，在新增segment初始化过程中，会有短暂的scheduled downtime，这个时间和GP集群服务重启时间差不多，并且与系统扩容前后大小无关。
> Uninterrupted service during expansion. Regular workloads, both scheduled and ad-hoc, are not
interrupted. A short, scheduled downtime period is required to initialize the new servers, similar to
downtime required to restart the system. The length of downtime is unrelated to the size of the system
before or after expansion

短暂的scheduled downtime会对业务造成什么样的影响呢？

### 2.1 生成扩容配置文件
```
# 交互输入要扩容的host，每个host上扩容的segment数目，以及mirror的方式(grouped, spread)
gpexpand -D zlytest
```
### 2.2 初始化扩容节点
```
[gpadmin@mdw1 ~]$ gpexpand -i gpexpand_inputfile_20161109_094342 -D zlytest
20161109:10:22:04:023158 gpexpand:mdw1:gpadmin-[INFO]:-local Greenplum Version: 'postgres (Greenplum Database) 4.3.99.00 build dev'
20161109:10:22:04:023158 gpexpand:mdw1:gpadmin-[INFO]:-master Greenplum Version: 'PostgreSQL 8.3.23 (Greenplum Database 4.3.99.00 build dev) on x86_64-unknown-linux-gnu, compiled by GCC gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4) compiled on Aug 23 2016 06:42:05'
20161109:10:22:05:023158 gpexpand:mdw1:gpadmin-[INFO]:-Querying gpexpand schema for current expansion state
20161109:10:22:06:023158 gpexpand:mdw1:gpadmin-[INFO]:-Readying Greenplum Database for a new expansion
20161109:10:24:34:023158 gpexpand:mdw1:gpadmin-[INFO]:-Checking database template1 for unalterable tables...
20161109:10:24:34:023158 gpexpand:mdw1:gpadmin-[INFO]:-Checking database postgres for unalterable tables...
20161109:10:24:34:023158 gpexpand:mdw1:gpadmin-[INFO]:-Checking database zlytest for unalterable tables...
20161109:10:24:34:023158 gpexpand:mdw1:gpadmin-[INFO]:-Checking database expand for unalterable tables...
20161109:10:24:34:023158 gpexpand:mdw1:gpadmin-[INFO]:-Checking database expand_t for unalterable tables...
20161109:10:24:34:023158 gpexpand:mdw1:gpadmin-[INFO]:-Checking database template1 for tables with unique indexes...
20161109:10:24:34:023158 gpexpand:mdw1:gpadmin-[INFO]:-Checking database postgres for tables with unique indexes...
20161109:10:24:34:023158 gpexpand:mdw1:gpadmin-[INFO]:-Checking database zlytest for tables with unique indexes...
20161109:10:24:34:023158 gpexpand:mdw1:gpadmin-[INFO]:-Checking database expand for tables with unique indexes...
20161109:10:24:34:023158 gpexpand:mdw1:gpadmin-[INFO]:-Checking database expand_t for tables with unique indexes...
20161109:10:24:35:023158 gpexpand:mdw1:gpadmin-[INFO]:-Syncing Greenplum Database extensions
20161109:10:24:35:023158 gpexpand:mdw1:gpadmin-[INFO]:-The packages on sdw1.com are consistent.
20161109:10:24:35:023158 gpexpand:mdw1:gpadmin-[INFO]:-The packages on sdw2.com are consistent.
20161109:10:24:36:023158 gpexpand:mdw1:gpadmin-[INFO]:-Creating segment template
20161109:10:24:36:023158 gpexpand:mdw1:gpadmin-[INFO]:-VACUUM FULL on the catalog tables
20161109:10:24:46:023158 gpexpand:mdw1:gpadmin-[INFO]:-Starting copy of segment dbid 1 to location /home/gpadmin/data/master/gpexpand_11092016_23158
20161109:10:24:47:023158 gpexpand:mdw1:gpadmin-[INFO]:-Copying postgresql.conf from existing segment into template
20161109:10:24:47:023158 gpexpand:mdw1:gpadmin-[INFO]:-Copying pg_hba.conf from existing segment into template
20161109:10:24:47:023158 gpexpand:mdw1:gpadmin-[INFO]:-Adding new segments into template pg_hba.conf
20161109:10:24:47:023158 gpexpand:mdw1:gpadmin-[INFO]:-Creating schema tar file
20161109:10:24:48:023158 gpexpand:mdw1:gpadmin-[INFO]:-Distributing template tar file to new hosts
20161109:10:24:51:023158 gpexpand:mdw1:gpadmin-[INFO]:-Configuring new segments (primary)
20161109:10:24:52:023158 gpexpand:mdw1:gpadmin-[INFO]:-Configuring new segments (mirror)
20161109:10:24:53:023158 gpexpand:mdw1:gpadmin-[INFO]:-Backing up pg_hba.conf file on original segments
20161109:10:24:53:023158 gpexpand:mdw1:gpadmin-[INFO]:-Copying new pg_hba.conf file to original segments
20161109:10:24:53:023158 gpexpand:mdw1:gpadmin-[INFO]:-Configuring original segments
20161109:10:24:53:023158 gpexpand:mdw1:gpadmin-[INFO]:-Cleaning up temporary template files
20161109:10:24:54:023158 gpexpand:mdw1:gpadmin-[INFO]:-Starting Greenplum Database in restricted mode
20161109:10:25:05:023158 gpexpand:mdw1:gpadmin-[INFO]:-Stopping database
20161109:10:25:27:023158 gpexpand:mdw1:gpadmin-[INFO]:-Checking if Transaction filespace was moved
20161109:10:25:27:023158 gpexpand:mdw1:gpadmin-[INFO]:-Checking if Temporary filespace was moved
20161109:10:25:27:023158 gpexpand:mdw1:gpadmin-[INFO]:-Configuring new segment filespaces
20161109:10:25:27:023158 gpexpand:mdw1:gpadmin-[INFO]:-Cleaning up databases in new segments.
20161109:10:25:27:023158 gpexpand:mdw1:gpadmin-[INFO]:-Starting master in utility mode
20161109:10:25:29:023158 gpexpand:mdw1:gpadmin-[INFO]:-Stopping master in utility mode
20161109:10:25:36:023158 gpexpand:mdw1:gpadmin-[INFO]:-Starting Greenplum Database in restricted mode
20161109:10:25:45:023158 gpexpand:mdw1:gpadmin-[INFO]:-Creating expansion schema
20161109:10:25:46:023158 gpexpand:mdw1:gpadmin-[INFO]:-Populating gpexpand.status_detail with data from database template1
20161109:10:25:48:023158 gpexpand:mdw1:gpadmin-[INFO]:-Populating gpexpand.status_detail with data from database postgres
20161109:10:25:49:023158 gpexpand:mdw1:gpadmin-[INFO]:-Populating gpexpand.status_detail with data from database zlytest
20161109:10:25:51:023158 gpexpand:mdw1:gpadmin-[INFO]:-Populating gpexpand.status_detail with data from database expand
20161109:10:25:52:023158 gpexpand:mdw1:gpadmin-[INFO]:-Populating gpexpand.status_detail with data from database expand_t
20161109:10:25:54:023158 gpexpand:mdw1:gpadmin-[INFO]:-Stopping Greenplum Database
20161109:10:26:17:023158 gpexpand:mdw1:gpadmin-[INFO]:-Starting Greenplum Database
20161109:10:26:26:023158 gpexpand:mdw1:gpadmin-[INFO]:-Starting new mirror segment synchronization
20161109:10:26:41:023158 gpexpand:mdw1:gpadmin-[INFO]:-************************************************
20161109:10:26:41:023158 gpexpand:mdw1:gpadmin-[INFO]:-Initialization of the system expansion complete.
20161109:10:26:41:023158 gpexpand:mdw1:gpadmin-[INFO]:-To begin table expansion onto the new segments
20161109:10:26:41:023158 gpexpand:mdw1:gpadmin-[INFO]:-rerun gpexpand
20161109:10:26:41:023158 gpexpand:mdw1:gpadmin-[INFO]:-************************************************
20161109:10:26:41:023158 gpexpand:mdw1:gpadmin-[INFO]:-Exiting...
20161109:10:26:41:023158 gpexpand:mdw1:gpadmin-[INFO]:-Cleaning up...
```
在运行扩容节点初始化过程中，在已有连接会话中进行查询，同时新建数据库连接。
```
# 在已有会话中查询
zlytest=# explain analyze select p_partkey,p_name,p_size from part_appendonly_col where p_size <= 7;
                                                                   QUERY PLAN                                                                    
-------------------------------------------------------------------------------------------------------------------------------------------------
 Gather Motion 4:1  (slice1; segments: 4)  (cost=0.00..3324806.00 rows=28000001 width=41)
   Rows out:  27991375 rows at destination with 3.086 ms to first row, 9052 ms to end, start offset by 18 ms.
   ->  Append-only Columnar Scan on part_appendonly_col  (cost=0.00..3324806.00 rows=7000001 width=41)
         Filter: p_size <= 7
         Rows out:  Avg 6997843.8 rows x 4 workers.  Max 7000723 rows (seg0) with 1.032 ms to first row, 8291 ms to end, start offset by 632 ms.
 Slice statistics:
   (slice0)    Executor memory: 318K bytes.
   (slice1)    Executor memory: 505K bytes avg x 4 workers, 505K bytes max (seg0).
 Statement statistics:
   Memory used: 128000K bytes
 Optimizer status: legacy query optimizer
 Total runtime: 10731.445 ms
(12 rows)

zlytest=# explain analyze explain analyze select p_partkey,p_name from part_appendonly_col where p_brand='Brand#11' or p_size=23; 
server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.
The connection to the server was lost. Attempting reset: Failed.
!> 
```

```
# 新建数据库连接
[gpadmin@mdw1 ~]$ psql zlytest
psql: FATAL:  the database system is shutting down
[gpadmin@mdw1 ~]$ psql zlytest
psql: FATAL:  the database system is shutting down
[gpadmin@mdw1 ~]$ psql zlytest
psql: could not connect to server: No such file or directory
	Is the server running locally and accepting
	connections on Unix domain socket "/tmp/.s.PGSQL.5432"?
```

从扩容节点初始化的日志中，可以看出在这个过程中是GP集群是有stop/start的操作的，在此过程中，新建的数据库连接无法连上，分为系统正在关闭和已关闭两个阶段；
已连接的会话，在系统正在关闭阶段还能够查询，系统关闭后连接断开。扩容节点初始化过程持续4分钟左右， 等扩容节点初始化完成后，可以新建数据库连接。

结合上述测试看到的现象，在扩容初始化过程中系统阻止新建查询，并等待当前所有会话的查询结束后关闭系统，在新增segment初始化操作完成后，启动系统服务。

### 2.3 数据重分布
```
gpexpand -D zlytest -d 5:00:00
```
重分布过程中，不中断已有查询，也能够新建数据库连接。

扩容状态记录在指定的数据库zlytest中，gpexpand.status记录各步骤执行状态，gpexpand.expansion_progress记录重分布状态信息，gpexpand.status_detail记录详细重分布状态信息，包括各表的重分布完成情况。
```
zlytest=# select * from gpexpand.
gpexpand.expansion_progress  gpexpand.status              gpexpand.status_detail    

zlytest=# select * from gpexpand.status;
      status       |          updated           
-------------------+----------------------------
 SETUP             | 2016-11-09 10:25:46.966165
 SETUP DONE        | 2016-11-09 10:25:54.250327
 EXPANSION STARTED | 2016-11-09 13:15:41.883765
(3 rows)

zlytest=# select * from gpexpand.expansion_progress;
             name             |    value     
------------------------------+--------------
 Estimated Expansion Rate     | 
 Tables In Progress           | 1
 Bytes In Progress            | 144409100288
 Bytes Left                   | 28849064848
 Tables Left                  | 7
 Estimated Time to Completion | 
(6 rows)

zlytest=# select * from gpexpand.status_detail ;
 dbname  |          fq_name           | schema_oid | table_oid | distribution_policy | distribution_policy_names | distribution_policy_coloids | storage_options | rank |   status    |     expansion_started     
 | expansion_finished | source_bytes 
---------+----------------------------+------------+-----------+---------------------+---------------------------+-----------------------------+-----------------+------+-------------+---------------------------
-+--------------------+--------------
 zlytest | public.supplier            |       2200 |     32814 | {1}                 | s_suppkey                 | 32814                       |                 |    2 | NOT STARTED |                           
 |                    |   1821605888
 zlytest | public.customer            |       2200 |     32875 | {1}                 | c_custkey                 | 32875                       |                 |    2 | NOT STARTED |                           
 |                    |            0
 zlytest | public.orders              |       2200 |     32907 | {1}                 | o_orderkey                | 32907                       |                 |    2 | NOT STARTED |                           
 |                    |            0
 zlytest | public.nation              |       2200 |     32939 | {1}                 | n_nationkey               | 32939                       |                 |    2 | NOT STARTED |                           
 |                    |       131072
 zlytest | public.region              |       2200 |     32969 | {1}                 | r_regionkey               | 32969                       |                 |    2 | NOT STARTED |                           
 |                    |       131072
 zlytest | public.part_appendonly_col |       2200 |     33224 | {1}                 | p_partkey                 | 33224                       |                 |    2 | NOT STARTED |                           
 |                    |  27027196816
 zlytest | public.supplier_row        |       2200 |     33281 | {1}                 | s_suppkey                 | 33281                       |                 |    2 | NOT STARTED |                           
 |                    |            0
 zlytest | public.partsupp            |       2200 |     32844 | {1}                 | ps_partkey                | 32844                       |                 |    2 | IN PROGRESS | 2016-11-09 13:15:43.818775
 |                    | 144409100288
(8 rows)
```

## 3 物理服务器扩容
环境准备：
1. PXE装机
2. 集群各机器/etc/hosts文件中添加新增服务器IP、主机名
3. master服务器上修改adb_scripts/deploy.conf，[meachines]中添加新增服务器主机名、IP，[roles]的segment里添加新增服务器主机名
4. 将master服务器adb_scripts拷贝至其他服务器及新增服务器，将adb_warehouse拷贝至新增服务器
3. 在新增服务器上运行python manual_entry -s os_env -a setup，配置环境
4. 交换root ssh
5. 将gpdb从master拷贝至新加入的服务器
6. 交换gpadmin ssh
7. 新增服务器上增加priamry、mirror数据目录，如/gpadmin/data/primay, /gpadmin/data/mirror，并将目录所有者改为gpadmin

扩容：
1. 生成扩容配置文件
```
# 生成扩容配置文件，交互输入要扩容的host，每个host上扩容的segment数目，以及mirror的方式(grouped, spread)
# 扩容过程中需要生成表，记录扩容运行情况，因此必须指定一个template1以外的数据库
gpexpand -D zlytest
```
2. 扩容节点初始化
```
# 根据配置文件扩容节点初始化
gpexpand -i gpexpand_inputfile_20161109_094342 -D zlytest
```
2. 数据重分布
```
# 在指定时间内完成重分布
gpexpand -D zlytest -d 1:00:00
```

### 3.1 物理服务器扩容测试
当前集群中有两台物理服务器seg1.com和seg2.con作为segment host，一共有8个primary segment，8个mirror segment. mirror方式采用grouped
```
zlytest=# SELECT * from gp_segment_configuration;
 dbid | content | role | preferred_role | mode | status | port  | hostname | address  | replication_port | san_mounts 
------+---------+------+----------------+------+--------+-------+----------+----------+------------------+------------
    1 |      -1 | p    | p              | s    | u      |  5432 | m1.com   | m1.com   |                  | 
    2 |       0 | p    | p              | s    | u      | 40000 | seg1.com | seg1.com |            41000 | 
    6 |       4 | p    | p              | s    | u      | 40000 | seg2.com | seg2.com |            41000 | 
    3 |       1 | p    | p              | s    | u      | 40001 | seg1.com | seg1.com |            41001 | 
    7 |       5 | p    | p              | s    | u      | 40001 | seg2.com | seg2.com |            41001 | 
    4 |       2 | p    | p              | s    | u      | 40002 | seg1.com | seg1.com |            41002 | 
    8 |       6 | p    | p              | s    | u      | 40002 | seg2.com | seg2.com |            41002 | 
    5 |       3 | p    | p              | s    | u      | 40003 | seg1.com | seg1.com |            41003 | 
    9 |       7 | p    | p              | s    | u      | 40003 | seg2.com | seg2.com |            41003 | 
   10 |       0 | m    | m              | s    | u      | 50000 | seg2.com | seg2.com |            51000 | 
   11 |       1 | m    | m              | s    | u      | 50001 | seg2.com | seg2.com |            51001 | 
   12 |       2 | m    | m              | s    | u      | 50002 | seg2.com | seg2.com |            51002 | 
   13 |       3 | m    | m              | s    | u      | 50003 | seg2.com | seg2.com |            51003 | 
   14 |       4 | m    | m              | s    | u      | 50000 | seg1.com | seg1.com |            51000 | 
   15 |       5 | m    | m              | s    | u      | 50001 | seg1.com | seg1.com |            51001 | 
   16 |       6 | m    | m              | s    | u      | 50002 | seg1.com | seg1.com |            51002 | 
   17 |       7 | m    | m              | s    | u      | 50003 | seg1.com | seg1.com |            51003 | 
   18 |      -1 | m    | m              | s    | u      |  5432 | m2.com   | m2.com   |                  | 
(18 rows)

```

#### 测试1
- segment级别扩容测试，尝试只在seg1.com上添加1个primary segment，1个mirror segment

生成配置文件，扩容的主机只输入seg1.com时，执行失败，提示至少输入两台主机。
```
[gpadmin@m1 ~]$ gpexpand -D zlytest
20161111:11:24:05:030128 gpexpand:m1:gpadmin-[INFO]:-local Greenplum Version: 'postgres (Greenplum Database) 4.3.99.00 build dev'
20161111:11:24:05:030128 gpexpand:m1:gpadmin-[INFO]:-master Greenplum Version: 'PostgreSQL 8.3.23 (Greenplum Database 4.3.99.00 build dev) on x86_64-unknown-linux-gnu, compiled by GCC gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4) compiled on Nov  1 2016 17:31:47'
20161111:11:24:05:030128 gpexpand:m1:gpadmin-[INFO]:-Querying gpexpand schema for current expansion state

System Expansion is used to add segments to an existing GPDB array.
gpexpand did not detect a System Expansion that is in progress.

Before initiating a System Expansion, you need to provision and burn-in
the new hardware.  Please be sure to run gpcheckperf/gpcheckos to make
sure the new hardware is working properly.

Please refer to the Admin Guide for more information.

Would you like to initiate a new System Expansion Yy|Nn (default=N):
> y

Enter a comma separated list of new hosts you want
to add to your array.  Do not include interface hostnames.
**Enter a blank line to only add segments to existing hosts**[]:
> seg1.com    
20161111:11:24:18:030128 gpexpand:m1:gpadmin-[ERROR]:-gpexpand failed: You must be adding two or more hosts when expanding a system with mirroring enabled. 

Exiting...
20161111:11:24:18:030128 gpexpand:m1:gpadmin-[INFO]:-Bringing Greenplum Database back online...
20161111:11:24:19:030128 gpexpand:m1:gpadmin-[INFO]:-Shutting down gpexpand...
20161111:11:24:19:030128 gpexpand:m1:gpadmin-[INFO]:-Cleaning up...
```
于是输入seg1.com,seg1.com，两个相同的主机名，执行成功，生成的配置文件却是在集群中的两台机器上都添加了个primary segment，1个mirror segment
```
[gpadmin@m1 ~]$ cat gpexpand_inputfile_20161111_112744
seg1.com:seg1.com:40004:/gpadmin/data/primary/gpseg8:19:8:p:41004
seg2.com:seg2.com:50004:/gpadmin/data/mirror/gpseg8:22:8:m:51004
seg2.com:seg2.com:40004:/gpadmin/data/primary/gpseg9:20:9:p:41004
seg1.com:seg1.com:50004:/gpadmin/data/mirror/gpseg9:21:9:m:51004
```

### 测试2
- 增加一台物理服务器seg3.com

当新增一台物理服务器seg3.com，无论mirror方式选择spread或grouped，都不能进行扩容，提示Not enough hosts for grouped mirroring
```
[gpadmin@m1 ~]$ cat new_hosts 
seg1.com
seg2.com
seg3.com

[gpadmin@m1 ~]$ gpexpand -f new_hosts -D zlytest
20161111:11:07:04:027039 gpexpand:m1:gpadmin-[INFO]:-local Greenplum Version: 'postgres (Greenplum Database) 4.3.99.00 build dev'
20161111:11:07:04:027039 gpexpand:m1:gpadmin-[INFO]:-master Greenplum Version: 'PostgreSQL 8.3.23 (Greenplum Database 4.3.99.00 build dev) on x86_64-unknown-linux-gnu, compiled by GCC gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4) compiled on Nov  1 2016 17:31:47'
20161111:11:07:05:027039 gpexpand:m1:gpadmin-[INFO]:-Querying gpexpand schema for current expansion state

System Expansion is used to add segments to an existing GPDB array.
gpexpand did not detect a System Expansion that is in progress.

Before initiating a System Expansion, you need to provision and burn-in
the new hardware.  Please be sure to run gpcheckperf/gpcheckos to make
sure the new hardware is working properly.

Please refer to the Admin Guide for more information.

Would you like to initiate a new System Expansion Yy|Nn (default=N):
> y

You must now specify a mirroring strategy for the new hosts.  Spread mirroring places
a given hosts mirrored segments each on a separate host.  You must be 
adding more hosts than the number of segments per host to use this. 
Grouped mirroring places all of a given hosts segments on a single 
mirrored host.  You must be adding at least 2 hosts in order to use this.



What type of mirroring strategy would you like?
 spread|grouped (default=grouped):
> grouped
20161111:11:07:12:027039 gpexpand:m1:gpadmin-[ERROR]:-gpexpand failed: Not enough hosts for grouped mirroring.  You must have at least 2 

Exiting...
20161111:11:07:12:027039 gpexpand:m1:gpadmin-[INFO]:-Bringing Greenplum Database back online...
20161111:11:07:12:027039 gpexpand:m1:gpadmin-[INFO]:-Shutting down gpexpand...
20161111:11:07:12:027039 gpexpand:m1:gpadmin-[INFO]:-Cleaning up...
```
此时，若选择集群中原有的seg1.com和seg2.com，可扩容，但选择seg3.com与任一主机，都不可扩容。

### 测试3
- 增加两台物理服务器seg3.com,seg4.com

#### 测试3-1
选择seg1.com,seg2.com,seg3.com,seg4.com做扩容，mirror方式为grouped，每台物理服务器上增加0个segment。能够做扩容，自动生成扩容配置文件。

生成的配置文件内容如下，在新增加的服务器seg3.com和seg4.com上分别添加4个primary，4个mirror。扩容完成后，所有服务器上的primary/mirror segment数目都是4。
```
[gpadmin@m1 ~]$ cat gpexpand_inputfile_20161111_134030 
seg3.com:seg3.com:40000:/gpadmin/data/primary/gpseg8:19:8:p:41000
seg4.com:seg4.com:50000:/gpadmin/data/mirror/gpseg8:31:8:m:51000
seg3.com:seg3.com:40001:/gpadmin/data/primary/gpseg9:20:9:p:41001
seg4.com:seg4.com:50001:/gpadmin/data/mirror/gpseg9:32:9:m:51001
seg3.com:seg3.com:40002:/gpadmin/data/primary/gpseg10:21:10:p:41002
seg4.com:seg4.com:50002:/gpadmin/data/mirror/gpseg10:33:10:m:51002
seg3.com:seg3.com:40003:/gpadmin/data/primary/gpseg11:22:11:p:41003
seg4.com:seg4.com:50003:/gpadmin/data/mirror/gpseg11:34:11:m:51003
seg4.com:seg4.com:40000:/gpadmin/data/primary/gpseg12:23:12:p:41000
seg3.com:seg3.com:50000:/gpadmin/data/mirror/gpseg12:27:12:m:51000
seg4.com:seg4.com:40001:/gpadmin/data/primary/gpseg13:24:13:p:41001
seg3.com:seg3.com:50001:/gpadmin/data/mirror/gpseg13:28:13:m:51001
seg4.com:seg4.com:40002:/gpadmin/data/primary/gpseg14:25:14:p:41002
seg3.com:seg3.com:50002:/gpadmin/data/mirror/gpseg14:29:14:m:51002
seg4.com:seg4.com:40003:/gpadmin/data/primary/gpseg15:26:15:p:41003
seg3.com:seg3.com:50003:/gpadmin/data/mirror/gpseg15:30:15:m:51003
```

#### 测试3-2
选择seg1.com,seg2.com,seg3.com,seg4.com做扩容，mirror方式为grouped，每台物理服务器上增加1个segment。能够做扩容，自动生成扩容配置文件。

以下是生成的扩容配置文件内容。在生成扩容阶段，每台服务器上增加的segment数输入的是1，集群中已有的seg1.com和seg2.com确实只分别增加了1个primary/mirror segment，
但新增的两个服务器上需要分别添加5个primary/mirror segment。扩容完成后，所有服务器上的primary/mirror segment数目都是5。
```
[gpadmin@m1 ~]$ cat gpexpand_inputfile_20161111_132725 
seg3.com:seg3.com:40000:/gpadmin/data/primary/gpseg8:19:8:p:41000
seg4.com:seg4.com:50000:/gpadmin/data/mirror/gpseg8:31:8:m:51000
seg3.com:seg3.com:40001:/gpadmin/data/primary/gpseg9:20:9:p:41001
seg4.com:seg4.com:50001:/gpadmin/data/mirror/gpseg9:32:9:m:51001
seg3.com:seg3.com:40002:/gpadmin/data/primary/gpseg10:21:10:p:41002
seg4.com:seg4.com:50002:/gpadmin/data/mirror/gpseg10:33:10:m:51002
seg3.com:seg3.com:40003:/gpadmin/data/primary/gpseg11:22:11:p:41003
seg4.com:seg4.com:50003:/gpadmin/data/mirror/gpseg11:34:11:m:51003
seg4.com:seg4.com:40000:/gpadmin/data/primary/gpseg12:23:12:p:41000
seg3.com:seg3.com:50000:/gpadmin/data/mirror/gpseg12:27:12:m:51000
seg4.com:seg4.com:40001:/gpadmin/data/primary/gpseg13:24:13:p:41001
seg3.com:seg3.com:50001:/gpadmin/data/mirror/gpseg13:28:13:m:51001
seg4.com:seg4.com:40002:/gpadmin/data/primary/gpseg14:25:14:p:41002
seg3.com:seg3.com:50002:/gpadmin/data/mirror/gpseg14:29:14:m:51002
seg4.com:seg4.com:40003:/gpadmin/data/primary/gpseg15:26:15:p:41003
seg3.com:seg3.com:50003:/gpadmin/data/mirror/gpseg15:30:15:m:51003
seg1.com:seg1.com:40004:/gpadmin/data/primary/gpseg16:35:16:p:41004
seg2.com:seg2.com:50004:/gpadmin/data/mirror/gpseg16:40:16:m:51004
seg2.com:seg2.com:40004:/gpadmin/data/primary/gpseg17:36:17:p:41004
seg3.com:seg3.com:50004:/gpadmin/data/mirror/gpseg17:41:17:m:51004
seg3.com:seg3.com:40004:/gpadmin/data/primary/gpseg18:37:18:p:41004
seg4.com:seg4.com:50004:/gpadmin/data/mirror/gpseg18:42:18:m:51004
seg4.com:seg4.com:40004:/gpadmin/data/primary/gpseg19:38:19:p:41004
seg1.com:seg1.com:50004:/gpadmin/data/mirror/gpseg19:39:19:m:51004

# 使用grep简单统计
# seg1.com上添加1个primary，1个mirror
# 添加后seg1.com上一共5个primay，5个mirror
[gpadmin@m1 ~]$ cat gpexpand_inputfile_20161111_132725 | grep seg1.com
seg1.com:seg1.com:40004:/gpadmin/data/primary/gpseg16:35:16:p:41004
seg1.com:seg1.com:50004:/gpadmin/data/mirror/gpseg19:39:19:m:51004

# seg1.com上增加一个primary,一个mirror
[gpadmin@m1 ~]$ cat gpexpand_inputfile_20161111_132725 | grep seg2.com
seg2.com:seg2.com:50004:/gpadmin/data/mirror/gpseg16:40:16:m:51004
seg2.com:seg2.com:40004:/gpadmin/data/primary/gpseg17:36:17:p:41004

# 新增的seg3.com上添加5个primary,5个mirror
[gpadmin@m1 ~]$ cat gpexpand_inputfile_20161111_132725 | grep seg3.com
seg3.com:seg3.com:40000:/gpadmin/data/primary/gpseg8:19:8:p:41000
seg3.com:seg3.com:40001:/gpadmin/data/primary/gpseg9:20:9:p:41001
seg3.com:seg3.com:40002:/gpadmin/data/primary/gpseg10:21:10:p:41002
seg3.com:seg3.com:40003:/gpadmin/data/primary/gpseg11:22:11:p:41003
seg3.com:seg3.com:50000:/gpadmin/data/mirror/gpseg12:27:12:m:51000
seg3.com:seg3.com:50001:/gpadmin/data/mirror/gpseg13:28:13:m:51001
seg3.com:seg3.com:50002:/gpadmin/data/mirror/gpseg14:29:14:m:51002
seg3.com:seg3.com:50003:/gpadmin/data/mirror/gpseg15:30:15:m:51003
seg3.com:seg3.com:50004:/gpadmin/data/mirror/gpseg17:41:17:m:51004
seg3.com:seg3.com:40004:/gpadmin/data/primary/gpseg18:37:18:p:41004

# 新增的seg4.com上添加5个primary,5个mirror
[gpadmin@m1 ~]$ cat gpexpand_inputfile_20161111_132725 | grep seg4.com
seg4.com:seg4.com:50000:/gpadmin/data/mirror/gpseg8:31:8:m:51000
seg4.com:seg4.com:50001:/gpadmin/data/mirror/gpseg9:32:9:m:51001
seg4.com:seg4.com:50002:/gpadmin/data/mirror/gpseg10:33:10:m:51002
seg4.com:seg4.com:50003:/gpadmin/data/mirror/gpseg11:34:11:m:51003
seg4.com:seg4.com:40000:/gpadmin/data/primary/gpseg12:23:12:p:41000
seg4.com:seg4.com:40001:/gpadmin/data/primary/gpseg13:24:13:p:41001
seg4.com:seg4.com:40002:/gpadmin/data/primary/gpseg14:25:14:p:41002
seg4.com:seg4.com:40003:/gpadmin/data/primary/gpseg15:26:15:p:41003
seg4.com:seg4.com:50004:/gpadmin/data/mirror/gpseg18:42:18:m:51004
seg4.com:seg4.com:40004:/gpadmin/data/primary/gpseg19:38:19:p:41004
```

#### 测试3-3
尝试通过自定义扩容配置文件，只在seg3.com和seg4.com上添加1个primary segment，1个mirror segment

配置文件如下：
```
[gpadmin@m1 ~]$ cat zly_gpexpand_inputfile 
seg3.com:seg3.com:40000:/gpadmin/data/primary/gpseg8:19:8:p:41000
seg4.com:seg4.com:50000:/gpadmin/data/mirror/gpseg8:31:8:m:51000
seg3.com:seg3.com:40001:/gpadmin/data/primary/gpseg9:20:9:p:41001
seg4.com:seg4.com:50001:/gpadmin/data/mirror/gpseg9:32:9:m:51001
```
运行扩容初始化命令：
```
[gpadmin@m1 ~]$ gpexpand -i zly_gpexpand_inputfile -D zlytest
20161111:14:09:56:001155 gpexpand:m1:gpadmin-[INFO]:-local Greenplum Version: 'postgres (Greenplum Database) 4.3.99.00 build dev'
20161111:14:09:56:001155 gpexpand:m1:gpadmin-[INFO]:-master Greenplum Version: 'PostgreSQL 8.3.23 (Greenplum Database 4.3.99.00 build dev) on x86_64-unknown-linux-gnu, compiled by GCC gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4) compiled on Nov  1 2016 17:31:47'
20161111:14:09:56:001155 gpexpand:m1:gpadmin-[INFO]:-Querying gpexpand schema for current expansion state
20161111:14:09:57:001155 gpexpand:m1:gpadmin-[INFO]:-Readying Greenplum Database for a new expansion
20161111:14:12:26:001155 gpexpand:m1:gpadmin-[INFO]:-Checking database template1 for unalterable tables...
20161111:14:12:26:001155 gpexpand:m1:gpadmin-[INFO]:-Checking database postgres for unalterable tables...
20161111:14:12:26:001155 gpexpand:m1:gpadmin-[INFO]:-Checking database zlytest for unalterable tables...
20161111:14:12:26:001155 gpexpand:m1:gpadmin-[INFO]:-Checking database template1 for tables with unique indexes...
20161111:14:12:26:001155 gpexpand:m1:gpadmin-[INFO]:-Checking database postgres for tables with unique indexes...
20161111:14:12:26:001155 gpexpand:m1:gpadmin-[INFO]:-Checking database zlytest for tables with unique indexes...
20161111:14:12:26:001155 gpexpand:m1:gpadmin-[ERROR]:-gpexpand failed: Invalid input file: Expansion segments have incorrect dbids 

Exiting...
20161111:14:12:26:001155 gpexpand:m1:gpadmin-[INFO]:-Bringing Greenplum Database back online...
20161111:14:14:40:001155 gpexpand:m1:gpadmin-[INFO]:-Shutting down gpexpand...
20161111:14:14:40:001155 gpexpand:m1:gpadmin-[INFO]:-Cleaning up..
```
扩容初始化失败，提示Expansion segments have incorrect dbids


### 总结
1. grouped mirror方式将A机上的所有mirror放到B机上，扩容时集群中已有的mirror segment是不会移动的，当只加入一台物理服务器做扩容时，这台服务器的mirror segment是没地方放的，
因此扩容至少新增2台或更多的物理服务器。spread mirror方式将A机的各个mirror放在不同的服务器上，因此要求主机数目比每台主机上的primary segment数目至少多1，目前没有测试这种方式的扩容。
2. 当前集群中每台物理服务器上有n个primary/mirror segment
- 每台服务器上增加的segment数输入的是0时，只对新增服务器扩容，新增服务器上添加n个primary segmet，n个mirror segment
- 每台服务器上增加的segment数输入的是m时(m>0)，集群原有服务器添加m个primary segmet，m个mirror segment，新增服务器添加(n+m)个primary segmet，(n+m)个mirror segment
3. 扩容的结果是每台物理服务器上的primary/mirror segment数目一致，不能通过自动生成扩容配置文件的方式达到只在新增服务器上添加数量小于n个segment
4. 第3条同样适用于segment级别的扩容